# TODO: complete this file.
# Classifier generation:
# Let N be the size of the training set.
# for each of t iterations:
    # sample N instances with replacement from the original training set.
    # apply the learning algorithm to the sample.
    # store the resulting classifier.

# Classification:
# for each of the t classifiers:
#    predict class of instance using classifier.
# return class that was predicted most often.

import numpy as np
from sklearn.impute import KNNImputer
from utils import *
import sys
from part_a.item_response import sigmoid, neg_log_likelihood, update_theta_beta
from part_a.neural_network import AutoEncoder, train
import torch
from torch.autograd import Variable
from scipy.sparse import csc_matrix

def sample_matrix(train_matrix):
    # randomly generate the row indices
    rand_mat_idx = np.random.randint(train_matrix.shape[0], size=train_matrix.shape[0])
    # reconstruct a train data matrix based on the row indices
    resample_mat = train_matrix[rand_mat_idx, :]
    return resample_mat

def sample_data(train_data):
    rand_idx = np.random.randint(len(train_data["is_correct"]), size=len(train_data["is_correct"]))
    data = {}
    data["user_id"] = [train_data["user_id"][i] for i in rand_idx]
    data["question_id"] = [train_data["question_id"][i] for i in rand_idx]
    data["is_correct"] = [train_data["is_correct"][i] for i in rand_idx]
    print(len(set(data["user_id"])), len(data["user_id"]))
    sys.exit(0)
    return data

def knn(train_matrix, test_data, k):
    predictions = []
    # random sampling the train data
    resample_mat = sample_matrix(train_matrix);
    # create a KNN model with number of neighbors = k
    nbrs = KNNImputer(n_neighbors=k)
    # fit and transform the KNN model using the randomly sampled train data matrix
    mat = nbrs.fit_transform(resample_mat.toarray())
    # predictions
    for i, _ in enumerate(test_data["is_correct"]):
        uid, qid = test_data["user_id"][i], test_data["question_id"][i]
        predictions.append(mat[uid, qid])
    return predictions

def item_response(train_data, test_data, lr, iterations):
    predictions = []
    resample_data = sample_data(train_data)
    theta = np.random.random(max(resample_data['user_id']) + 1)
    beta = np.random.random(max(resample_data['question_id']) + 1)

    for i in range(iterations):
        theta, beta = update_theta_beta(resample_data, lr, theta, beta)

    for i, _ in enumerate(test_data["is_correct"]):
        uid, qid = test_data["user_id"][i], test_data["question_id"][i]
        x = (theta[uid] - beta[qid]).sum()
        p_a = sigmoid(x)
        predictions.append(p_a)

    return predictions

def nn(train_matrix, test_data, lr, epochs, lamb):
    predictions = []
    resample_mat = sample_matrix(train_matrix).toarray()
    model = AutoEncoder(resample_mat.shape[1])
    zero_train_matrix = resample_mat.copy()
    zero_train_matrix[np.isnan(resample_mat)] = 0
    zero_train_matrix = torch.FloatTensor(zero_train_matrix)
    resample_mat = torch.FloatTensor(resample_mat)
    train(model, lr, lamb, resample_mat, zero_train_matrix, None, epochs, None)
    for i, _ in enumerate(test_data["is_correct"]):
        uid, qid = test_data["user_id"][i], test_data["question_id"][i]
        inputs = Variable(zero_train_matrix[uid]).unsqueeze(0)
        outputs = model(inputs)
        predictions.append(outputs[0][qid].item())
    return predictions

def evaluate(predictions, test_data):
    threshold = 0.5
    correct = 0
    for i, v in enumerate(predictions):
        if v >= threshold and test_data["is_correct"][i]:
            correct += 1
        if v < threshold and not test_data["is_correct"][i]:
            correct += 1
    return correct / len(predictions)

def ensemble (train_matrix, train_data, test_data):
    # # create a list to store all the probabilities generated by 3 base models
    # prob_all = []
    # # build 3 classifiers
    # for n in range(3):

    # try different models
    knn_pred = knn(train_matrix, test_data, 16)
    item_response_pred = item_response(train_data, test_data, 0.005, 10)
    nn_pred = nn(train_matrix, test_data, 0.01, 35, 0.001)
    print("knn accuracy:", evaluate(knn_pred, test_data))
    print("item response accuracy:", evaluate(item_response_pred, test_data))
    print("nn accuracy:", evaluate(nn_pred, test_data))
    tot = [knn_pred[i] + item_response_pred[i] + nn_pred[i] for i in range(len(test_data["is_correct"]))]
    bagging = [v/3 for v in tot]
    print("bagging accuracy:", evaluate(bagging, test_data))


#########

        # average probability of 3 model outputs
    #     predictions = []
    #     for i in range(len(test_data["user_id"])):
    #         cur_user_id = test_data["user_id"][i]
    #         cur_question_id = test_data["question_id"][i]
    #         predictions.append(mat[cur_user_id, cur_question_id])
    #     # store the prediction of each model into a list
    #     prob_all.append(predictions)
    # prob_all = np.asarray(prob_all)

    # # average the probability computed by the 3 models
    # prob_avg = np.mean(prob_all, axis=0)
    # pred = []
    # threshold = 0.5
    # total_accurate = 0
    # # classify the data and compute the accuracy
    # for i in range(len(prob_avg)):
    #     if prob_avg[i] < threshold:
    #         pred.append(0)
    #         if test_data['is_correct'][i]:
    #             total_accurate += 0
    #         else:
    #             total_accurate += 1
    #     else:
    #         pred.append(1)
    #         if test_data['is_correct'][i]:
    #             total_accurate += 1
    #         else:
    #             total_accurate += 0
    # acc = total_accurate / float(len(prob_avg))
    ##################

    # average binary outputs of three models
        # create a list to store the predictions for each model
    #     predictions = []
    #     threshold = 0.5
    #     total_accurate = 0
    #     for i in range(len(test_data["user_id"])):
    #         cur_user_id = test_data["user_id"][i]
    #         cur_question_id = test_data["question_id"][i]
    #         if mat[cur_user_id, cur_question_id] >= threshold:
    #             predictions.append(1)
    #         else:
    #             predictions.append(0)
    #     # store the prediction of each model into a list
    #     prob_all.append(predictions)
    # prob_all = np.asarray(prob_all)
    # prob_avg = np.mean(prob_all, axis=0)
    # for i in range (len(prob_avg)):
    #     if prob_avg[i] >= threshold and test_data['is_correct'][i]:
    #         total_accurate += 1
    #     if prob_avg[i] < threshold and not test_data['is_correct'][i]:
    #         total_accurate += 1
    # acc = total_accurate / float(len(prob_avg))
    # return acc

if __name__ == '__main__':
    train_data = load_train_csv("../data")
    sparse_matrix = load_train_sparse("../data")
    val_data = load_valid_csv("../data")
    test_data = load_public_test_csv("../data")
    val_acc = ensemble(sparse_matrix, train_data, val_data)
    # test_acc = ensemble(sparse_matrix, test_data)
    # print(sparse_matrix)
    # print(train_data.keys())


